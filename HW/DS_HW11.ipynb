{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW 11 AmirHossein Naghdi 400102169\n"
      ],
      "metadata": {
        "id": "bUZwAGp06rlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15 Points on the notebook running correctly."
      ],
      "metadata": {
        "id": "kL6zs7By9uAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15 Points on having sufficient explanations and overall readability of the notebook"
      ],
      "metadata": {
        "id": "Eucga3S_-Kt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG19, ResNet50\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg_preprocess\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_data = np.concatenate((x_train, x_test))\n",
        "y_data = np.concatenate((y_train, y_test))\n",
        "x_data = x_data.astype(\"float32\") / 255.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj82T7De6vyO",
        "outputId": "8c9c4fb9-bc20-4e0c-c7f2-576735973958"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 Points: Creating a convolutional network with Keras (with at least two layers of convolution layer)"
      ],
      "metadata": {
        "id": "cpL7ZDOk9v_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cross_validation(build_model, preprocess=None, epochs=5, batch_size=64):\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    acc_list = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(x_data):\n",
        "        x_train, x_val = x_data[train_idx], x_data[val_idx]\n",
        "        y_train_, y_val_ = y_data[train_idx], y_data[val_idx]\n",
        "\n",
        "        y_train = tf.keras.utils.to_categorical(y_train_, 10)\n",
        "        y_val = tf.keras.utils.to_categorical(y_val_, 10)\n",
        "\n",
        "        if preprocess:\n",
        "            x_train = preprocess(x_train)\n",
        "            x_val = preprocess(x_val)\n",
        "\n",
        "        model = build_model()\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        _, acc = model.evaluate(x_val, y_val, verbose=0)\n",
        "        acc_list.append(acc)\n",
        "\n",
        "    print(f\"Average Accuracy: {np.mean(acc_list):.4f}\")\n",
        "    return np.mean(acc_list)\n"
      ],
      "metadata": {
        "id": "crzHdz7d6yGW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_cnn(kernel_size=3):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (kernel_size, kernel_size), activation='relu', input_shape=(32, 32, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (kernel_size, kernel_size), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "for k in [3, 5, 7]:\n",
        "    print(f\"\\nTesting kernel size {k}\")\n",
        "    run_cross_validation(lambda: build_cnn(kernel_size=k))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4r9BNmj619O",
        "outputId": "af45d325-e105-44d0-caef-54f0fc0cf822"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing kernel size 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.6445\n",
            "\n",
            "Testing kernel size 5\n",
            "Average Accuracy: 0.6418\n",
            "\n",
            "Testing kernel size 7\n",
            "Average Accuracy: 0.5652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20 Points: Tuning the above network for:\n",
        "* 5 Points: Tuning the kernel size (i.e. the size of the receptive field) for convolutional layers\n",
        "* 5 Points: Tuning the stride for convolutional layers\n",
        "* 5 Points: Tuning the pooling size (i.e. the size of the receptive field) for pooling layers\n",
        "* 5 Points: Tuning the stride for pooling layers"
      ],
      "metadata": {
        "id": "lkUOZ12w93Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn_advanced(conv_stride=1, pool_size=2, pool_stride=2):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), strides=conv_stride, activation='relu', input_shape=(32, 32, 3)),\n",
        "        layers.MaxPooling2D(pool_size=(pool_size, pool_size), strides=pool_stride),\n",
        "        layers.Conv2D(64, (3, 3), strides=conv_stride, activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(pool_size, pool_size), strides=pool_stride),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "configs = [\n",
        "    (1, 2, 2),\n",
        "    (2, 2, 2),\n",
        "    (1, 3, 2)\n",
        "]\n",
        "\n",
        "for conv_stride, pool_size, pool_stride in configs:\n",
        "    print(f\"\\nStride={conv_stride}, Pool Size={pool_size}, Pool Stride={pool_stride}\")\n",
        "    run_cross_validation(lambda: build_cnn_advanced(conv_stride, pool_size, pool_stride))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He5POL6X66EH",
        "outputId": "1e188270-cd98-4a12-c5bc-208613e0ee0d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stride=1, Pool Size=2, Pool Stride=2\n",
            "Average Accuracy: 0.6628\n",
            "\n",
            "Stride=2, Pool Size=2, Pool Stride=2\n",
            "Average Accuracy: 0.4612\n",
            "\n",
            "Stride=1, Pool Size=3, Pool Stride=2\n",
            "Average Accuracy: 0.6489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 Points: Perform data augmentation and train your model above using the ImageGenerator class"
      ],
      "metadata": {
        "id": "IxDu9gNF-CIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "def build_augmented_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "kf = KFold(n_splits=3, shuffle=True)\n",
        "for train_idx, val_idx in kf.split(x_data):\n",
        "    x_train, x_val = x_data[train_idx], x_data[val_idx]\n",
        "    y_train = tf.keras.utils.to_categorical(y_data[train_idx], 10)\n",
        "    y_val = tf.keras.utils.to_categorical(y_data[val_idx], 10)\n",
        "\n",
        "    model = build_augmented_model()\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(datagen.flow(x_train, y_train, batch_size=64), epochs=5, verbose=0)\n",
        "    _, acc = model.evaluate(x_val, y_val, verbose=0)\n",
        "    print(\"Augmented Fold Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaKastC867-G",
        "outputId": "e9e03d98-8f4a-4f9a-926c-076acb69d34d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented Fold Accuracy: 0.6376000046730042\n",
            "Augmented Fold Accuracy: 0.6402000188827515\n",
            "Augmented Fold Accuracy: 0.628849983215332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20 Points: Perform transfer learning using two of the available models in Keras applications (e.g. VGG19, ResNet, EfficientNet, etc.)"
      ],
      "metadata": {
        "id": "ZIjdHGbT-Hxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transfer_model(base_model, preprocess):\n",
        "    base = base_model(include_top=False, weights='imagenet', input_shape=(32, 32, 3))\n",
        "    base.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        layers.Lambda(lambda x: preprocess(x)),\n",
        "        base,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "print(\"\\nTransfer Learning with VGG19\")\n",
        "run_cross_validation(lambda: build_transfer_model(VGG19, vgg_preprocess))\n",
        "\n",
        "print(\"\\nTransfer Learning with ResNet50\")\n",
        "run_cross_validation(lambda: build_transfer_model(ResNet50, resnet_preprocess))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htjteBfa7Dz1",
        "outputId": "2e89a043-add7-4304-fdf5-35a53668bb8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transfer Learning with VGG19\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "Average Accuracy: 0.2297\n",
            "\n",
            "Transfer Learning with ResNet50\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "Average Accuracy: 0.3291\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.32910000284512836)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 Points: Express you opnion about the effects of the window size (i.e. receptive field) in convolution layers on the performance of neural network. In other words, what happens if we increase or decrease the size of the receptive field? and Why?"
      ],
      "metadata": {
        "id": "sf9kAXEy-Nat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n37U2sPw-O0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is the effect of kernel (receptive field) size on performance?\n",
        "\n",
        "Increasing the receptive field (kernel size) can help capture more global features,\n",
        "but may lead to overfitting on small datasets or loss of local detail. Smaller kernels\n",
        "are more effective in early layers, capturing fine textures. Larger kernels can be\n",
        "useful in deeper layers where abstract features are learned.\n",
        "\n",
        "Balance is key: too small → underfitting, too large → overfitting or inefficient.\n"
      ],
      "metadata": {
        "id": "h3IiQPhY7UdF"
      }
    }
  ]
}