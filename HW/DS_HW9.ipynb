{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW9 AmirHossein Naghdi - 400102169"
      ],
      "metadata": {
        "id": "9fw_95Mjo4I0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15 Points on the notebook running correctly."
      ],
      "metadata": {
        "id": "yvPzRi51pF5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15 Points on having sufficient explanations and overall readability of the notebook"
      ],
      "metadata": {
        "id": "88-An0eerDJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 Points: Multilayer Perceptron with Scikit-Learn\n",
        "* 5 Points: binary classification with F1-score above 0.75\n",
        "* 5 Points: regression with R2-score above 0.8"
      ],
      "metadata": {
        "id": "UumluKnRpG_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "X_classification, y_classification = load_breast_cancer(return_X_y=True)"
      ],
      "metadata": {
        "id": "7NvfkZA7pPuN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target"
      ],
      "metadata": {
        "id": "3ad3hgThpUXs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classification\n",
        "# Multi-Layer Perceptron (MLP) for Classification\n",
        "The MLP is a type of feedforward neural network used for supervised learning tasks like classification. It consists of an input layer, one or more hidden layers with non-linear activation functions, and an output layer.\n",
        "\n",
        "It works by:\n",
        "\n",
        "Passing input data through the network (forward propagation),\n",
        "\n",
        "Computing a loss (e.g., cross-entropy for classification),\n",
        "\n",
        "Adjusting weights using backpropagation and an optimizer (like stochastic gradient descent),\n",
        "\n",
        "Iterating this process to minimize the loss and improve prediction accuracy.\n",
        "\n",
        "In essence, MLP learns to map inputs to outputs by adjusting internal parameters based on labeled training data."
      ],
      "metadata": {
        "id": "4jqxW8qapYuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_classification, y_classification, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1 Score (Classification): {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRddBVLko_2N",
        "outputId": "690d87c6-1d94-497a-9307-de042ea03797"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score (Classification): 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Regression\n",
        "# Multi-Layer Perceptron (MLP) for Regression\n",
        "The MLPRegressor is a type of feedforward neural network used for supervised regression tasks, where the goal is to predict continuous values.\n",
        "\n",
        "It works by:\n",
        "\n",
        "Passing input features through multiple hidden layers with non-linear activation functions,\n",
        "\n",
        "Computing a loss (typically mean squared error) between predicted and true values,\n",
        "\n",
        "Updating weights using backpropagation and an optimization algorithm (e.g., Adam or SGD),\n",
        "\n",
        "Iteratively improving predictions over training epochs.\n",
        "\n",
        "The model learns a complex, non-linear mapping from input features to a continuous output. The depth and size of the hidden layers (e.g., (64, 32, 16)) determine the model's capacity to capture patterns in the data.\n",
        "\n",
        "The performance is commonly evaluated using metrics like R² score, which measures how well the predictions explain the variance in the target variable."
      ],
      "metadata": {
        "id": "VBrbL6aXpj2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(64, 32, 16), max_iter=500, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R2 Score (Scikit-Learn MLP): {r2:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR0_nEnnpjZd",
        "outputId": "3fd0454f-de62-4811-b391-bd31b124f4c2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Score (Scikit-Learn MLP): 0.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15 Points: 4-layer feedforward network with Keras\n",
        "* 10 Points: binary classification with F1-score above 0.75\n",
        "* 5 Points: regression with R2-score above 0.8"
      ],
      "metadata": {
        "id": "GSXc29tOpqr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classification\n",
        "# Binary Classification with Keras Feedforward Neural Network\n",
        "This model is a feedforward neural network (a type of Multi-Layer Perceptron) built using Keras for a binary classification task—predicting whether a tumor is malignant or benign using the Breast Cancer dataset.\n",
        "\n",
        "It works by:\n",
        "\n",
        "Constructing a network with an input layer, three hidden layers (using ReLU activation), and a sigmoid-activated output layer that outputs a probability for the positive class.\n",
        "\n",
        "Using binary cross-entropy as the loss function, which is appropriate for binary classification tasks.\n",
        "\n",
        "Optimizing the model via Adam, a popular gradient-based optimizer, through backpropagation.\n",
        "\n",
        "After training for multiple epochs, it outputs a probability for each input, which is then thresholded at 0.5 to assign class labels.\n",
        "\n",
        "The model learns non-linear patterns in the input features to distinguish between the two classes, and its performance is evaluated using the F1-score, which balances precision and recall."
      ],
      "metadata": {
        "id": "jXtR5pALpurk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
        "\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\").flatten()\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1 Score (Binary Classification): {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6V8MybLptXm",
        "outputId": "baf1b67b-d1c6-4fc2-91f8-d62dca87c73a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "F1 Score (Binary Classification): 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# regression\n",
        "# Regression with Keras Feedforward Neural Network\n",
        "This model is a Keras-based feedforward neural network used for a regression task on the California Housing dataset, where the goal is to predict continuous housing prices.\n",
        "\n",
        "It works by:\n",
        "\n",
        "Building a multi-layer perceptron with an input layer, three hidden layers (using ReLU activation), and an output layer with no activation (linear) to produce continuous predictions.\n",
        "\n",
        "Using mean squared error (MSE) as the loss function, which is standard for regression problems.\n",
        "\n",
        "Optimizing the network using the Adam optimizer, which adapts learning rates during training based on gradient estimates.\n",
        "\n",
        "Learning to approximate the underlying relationship between input features and target values through forward propagation and backpropagation over multiple epochs.\n",
        "\n",
        "The model is evaluated using the R² score, which indicates how well the predictions explain the variance in the actual housing prices."
      ],
      "metadata": {
        "id": "8YyFwuntpweF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R2 Score (Keras NN): {r2:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67dXpBrDpxaM",
        "outputId": "9aa03677-a013-4995-9d69-42fef83fdaa4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "R2 Score (Keras NN): 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#20 Points: 4-layer feedforward network with PyTorch\n",
        "* 10 Points: binary classification with F1-score above 0.75\n",
        "* 10 Points: regression with R2-score above 0.8"
      ],
      "metadata": {
        "id": "uz0XOzQMp7H0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classification\n",
        "# Binary Classification with PyTorch Feedforward Neural Network\n",
        "This is a binary classification model built using PyTorch, applied to the Breast Cancer dataset.\n",
        "\n",
        "It works by:\n",
        "\n",
        "Defining a feedforward neural network with an input layer, multiple hidden layers using ReLU activation, and a sigmoid-activated output layer that produces probabilities between 0 and 1.\n",
        "\n",
        "Using binary cross-entropy loss (BCELoss) to measure the difference between predicted probabilities and true binary labels.\n",
        "\n",
        "Training the model using Adam optimizer through backpropagation over several epochs.\n",
        "\n",
        "Making predictions on test data by thresholding the sigmoid output at 0.5 to assign binary class labels.\n",
        "\n",
        "The network learns to distinguish between the two classes based on patterns in the input features, and its performance is assessed using the F1-score, which balances precision and recall."
      ],
      "metadata": {
        "id": "1mKUcTX_p9dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(X.shape[1], 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = Classifier()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test_tensor).numpy()\n",
        "    y_pred_labels = (y_pred > 0.5).astype(int)\n",
        "    f1 = f1_score(y_test, y_pred_labels)\n",
        "    print(f\"F1 Score (PyTorch Classification): {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgiYHI0Zp9ki",
        "outputId": "0f82c2f5-d228-41bb-9201-0b923cd480ab"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score (PyTorch Classification): 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# regression\n",
        "# Regression with PyTorch Feedforward Neural Network\n",
        "This is a regression model implemented in PyTorch, applied to the California Housing dataset to predict continuous housing prices.\n",
        "\n",
        "It works by:\n",
        "\n",
        "Building a deep feedforward neural network with several hidden layers using ReLU activation and a final linear output layer to produce continuous predictions.\n",
        "\n",
        "Using mean squared error (MSE) as the loss function to quantify the difference between predicted and actual values.\n",
        "\n",
        "Training the model using the Adam optimizer and backpropagation, which updates the model's weights to minimize the MSE over training epochs.\n",
        "\n",
        "Evaluating model performance using the R² score, which reflects how well the model explains the variance in housing prices.\n",
        "\n",
        "The network learns complex, non-linear relationships in the input features to produce accurate real-valued predictions."
      ],
      "metadata": {
        "id": "hJ0I8hcap9p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "class BetterNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BetterNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(8, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        return self.fc4(x)\n",
        "\n",
        "model = BetterNet()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for epoch in range(200):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train_tensor)\n",
        "    loss = loss_fn(output, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test_tensor).numpy().flatten()\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Improved R² Score (PyTorch): {r2:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au9KS2w0p9vN",
        "outputId": "b5b11627-53ae-4fdb-9f52-3311b2fd0531"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improved R² Score (PyTorch): 0.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15 Points: 4-layer non-sequential feedforward network with Keras\n",
        "* 5 Points: binary classification with F1-score above 0.75\n",
        "* 5 Points: regression with R2-score above 0.8"
      ],
      "metadata": {
        "id": "MYxXOHZpqKKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classification\n",
        "# Binary Classification using Keras Functional API\n",
        "This model is a binary classification network implemented using the Keras Functional API, applied to the Breast Cancer dataset.\n",
        "\n",
        "It works by:\n",
        "\n",
        "Defining the model structure using the Functional API:\n",
        "\n",
        "The input layer accepts data with a shape matching the number of features (X.shape[1]).\n",
        "\n",
        "Multiple hidden layers with ReLU activation help learn complex features.\n",
        "\n",
        "A final sigmoid-activated output layer produces probabilities for binary classification.\n",
        "\n",
        "Using binary cross-entropy loss to measure the difference between predicted probabilities and true binary labels.\n",
        "\n",
        "Optimizing the model with the Adam optimizer, which adjusts learning rates during training.\n",
        "\n",
        "Incorporating precision and recall as metrics to evaluate the classification performance, in addition to using the F1 score for final evaluation.\n",
        "\n",
        "The model is trained on the training data and evaluated on the test set by thresholding the predicted probabilities at 0.5 to generate class labels, and its performance is assessed using the F1 score, balancing both precision and recall."
      ],
      "metadata": {
        "id": "T33nV8EgqMT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "inputs = Input(shape=(X.shape[1],))\n",
        "x = Dense(64, activation='relu')(inputs)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=[Precision(), Recall()])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=100, verbose=0, batch_size=32)\n",
        "\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1 Score (Keras Non-Sequential Classification): {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddh1YNgvqMZD",
        "outputId": "6b99b5fa-5dd8-4152-8ebe-81cd3f7f5b94"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "F1 Score (Keras Non-Sequential Classification): 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# regression\n",
        "# Regression using Keras Functional API\n",
        "This model is a regression network built using the Keras Functional API, applied to the California Housing dataset to predict continuous housing prices.\n",
        "\n",
        "It works by:\n",
        "\n",
        "Defining the model structure using the Functional API:\n",
        "\n",
        "The input layer accepts data with a shape corresponding to the number of features (X.shape[1]).\n",
        "\n",
        "Several hidden layers with ReLU activation enable the model to learn complex patterns in the data.\n",
        "\n",
        "The final output layer is a linear activation (default), which allows the model to output continuous predictions.\n",
        "\n",
        "Using mean squared error (MSE) as the loss function to measure the difference between the predicted and actual values, which is common for regression tasks.\n",
        "\n",
        "Optimizing the model using the Adam optimizer, which adjusts the learning rate during training.\n",
        "\n",
        "The model is trained on the training data and evaluated on the test set using the R² score, which indicates how well the model’s predictions explain the variance in the target values (housing prices)."
      ],
      "metadata": {
        "id": "qNMql4HEqMd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "inputs = Input(shape=(X.shape[1],))\n",
        "x = Dense(64, activation='relu')(inputs)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "outputs = Dense(1)(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer=Adam(0.001), loss='mse')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=100, verbose=0, batch_size=32)\n",
        "\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R² Score (Keras Non-Sequential Regression): {r2:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDopPQ-4qMim",
        "outputId": "1ea5a7f2-148d-4e45-cbec-4cfa9cff8014"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "R² Score (Keras Non-Sequential Regression): 0.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus 15 Points (if dataset has time-series like features) 3-layer Recurrent Neural Network with Keras\n",
        "* 10 Points: binary classification with F1-score above 0.75\n",
        "* 5 Points: regression with R2-score above 0.8"
      ],
      "metadata": {
        "id": "bHRBL9f1qtTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classification\n",
        "# Binary Classification with RNN on Breast Cancer Dataset\n",
        "This model uses a Recurrent Neural Network (RNN) to perform binary classification on the Breast Cancer dataset, predicting whether tumors are malignant or benign.\n",
        "\n",
        "It works by:\n",
        "\n",
        "Preprocessing:\n",
        "\n",
        "The features are normalized using MinMaxScaler to scale values between 0 and 1.\n",
        "\n",
        "The data is reshaped to fit the requirements of an RNN. Here, the data is divided into timesteps (10) and features per step (3), with padding if necessary.\n",
        "\n",
        "RNN Architecture:\n",
        "\n",
        "The model includes three SimpleRNN layers, each with 64 units and tanh activation.\n",
        "\n",
        "Dropout layers are added to prevent overfitting.\n",
        "\n",
        "The output layer uses a sigmoid activation to output probabilities for binary classification.\n",
        "\n",
        "Training:\n",
        "\n",
        "The model is trained using the Adam optimizer and binary cross-entropy loss for binary classification tasks.\n",
        "\n",
        "The model is trained for 40 epochs with a validation split of 0.2, which helps monitor performance on unseen data during training.\n",
        "\n",
        "Evaluation:\n",
        "\n",
        "After training, the model predicts probabilities on the test set, and the predicted probabilities are thresholded at 0.5 to assign class labels.\n",
        "\n",
        "The performance is evaluated using the F1-score and the classification report, which provides detailed metrics such as precision, recall, and F1-score.\n",
        "\n",
        "The RNN model leverages temporal relationships in the input features and performs well on the classification task, with the F1-score serving as the primary evaluation metric."
      ],
      "metadata": {
        "id": "qJJvG5cwqtzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_padded = np.zeros((X.shape[0], 10 * 3))\n",
        "X_padded[:, :X.shape[1]] = X\n",
        "X_rnn = X_padded.reshape(-1, 10, 3)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_rnn, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential([\n",
        "    SimpleRNN(64, activation='tanh', return_sequences=True, input_shape=(10, 3)),\n",
        "    Dropout(0.3),\n",
        "    SimpleRNN(64, activation='tanh', return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    SimpleRNN(64, activation='tanh'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.2, verbose=0)\n",
        "\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1-score: {f1:.2f}\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG066Rqlqt7Q",
        "outputId": "587a345c-6105-4c8b-ee15-b52acd53f438"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step\n",
            "F1-score: 0.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94        43\n",
            "           1       0.97      0.96      0.96        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.95      0.96      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# regression\n",
        "# Regression with RNN on California Housing Dataset\n",
        "This model applies a Recurrent Neural Network (RNN) to perform regression on the California Housing dataset, aiming to predict continuous housing prices.\n",
        "\n",
        "It works by:\n",
        "\n",
        "Preprocessing:\n",
        "\n",
        "The features are normalized using MinMaxScaler, ensuring all input values are scaled between 0 and 1.\n",
        "\n",
        "The data is reshaped to fit the RNN input format, with 4 timesteps and 2 features per timestep, filling extra space with zeros to ensure uniform input size.\n",
        "\n",
        "RNN Architecture:\n",
        "\n",
        "The model consists of three SimpleRNN layers, each with 64 units and ReLU activation.\n",
        "\n",
        "Dropout layers are added to prevent overfitting by randomly setting a fraction of input units to zero during training.\n",
        "\n",
        "The final Dense layer provides the output with no activation function, as it is a regression task predicting continuous values.\n",
        "\n",
        "Training:\n",
        "\n",
        "The model is compiled with the Adam optimizer and mean squared error (MSE) loss function, as MSE is a standard loss function for regression tasks.\n",
        "\n",
        "The model is trained for 50 epochs with a validation split of 0.2 to assess its performance on unseen data.\n",
        "\n",
        "Evaluation:\n",
        "\n",
        "After training, predictions are made on the test set, and the performance is evaluated using R² score and mean squared error (MSE).\n",
        "\n",
        "The R² score indicates how well the model's predictions explain the variance in the target values (housing prices), while MSE measures the average squared difference between predicted and actual values.\n",
        "\n",
        "This RNN model can capture temporal dependencies in the data, although its primary advantage in this context might be in handling sequential patterns that could emerge in time-based data, even though the California Housing dataset is typically non-sequential."
      ],
      "metadata": {
        "id": "cQEspDezquCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "X = MinMaxScaler().fit_transform(X)\n",
        "\n",
        "X_padded = np.zeros((X.shape[0], 8))\n",
        "X_padded[:, :X.shape[1]] = X\n",
        "X_rnn = X_padded.reshape(-1, 4, 2)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_rnn, y, test_size=0.2, random_state=42)\n",
        "\n",
        "reg_model = Sequential([\n",
        "    SimpleRNN(64, activation='relu', return_sequences=True, input_shape=(4, 2)),\n",
        "    Dropout(0.3),\n",
        "    SimpleRNN(64, activation='relu', return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    SimpleRNN(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "reg_model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "reg_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
        "\n",
        "y_pred = reg_model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"R2-score: {r2:.2f}\")\n",
        "print(f\"MSE: {mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZOV0WZKquGf",
        "outputId": "76be34e8-cb94-468c-8b62-32fc9569e4d2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "R2-score: 0.75\n",
            "MSE: 0.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 Points: Explain why neural networks are so powerful and what the diffcult part is in designing neural networks."
      ],
      "metadata": {
        "id": "DfFZl3M-rcOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Neural Networks Are Powerful\n",
        "\n",
        "1. **Universal Function Approximation**  \n",
        "   Neural networks can approximate any continuous function given sufficient neurons and layers (Universal Approximation Theorem).\n",
        "\n",
        "2. **Representation Learning**  \n",
        "   They automatically learn useful features from raw data, reducing the need for manual feature engineering. This is essential in tasks like image classification, natural language processing, and speech recognition.\n",
        "\n",
        "3. **Scalability with Data and Compute**  \n",
        "   Neural networks generally perform better with more data and compute resources, which allows them to scale well with modern hardware and large datasets.\n",
        "\n",
        "4. **Flexibility Across Domains**  \n",
        "   Neural networks are adaptable to a wide range of tasks and data types, such as images, text, audio, and structured data. Architectures like CNNs, RNNs, and transformers are applicable across many domains.\n",
        "\n",
        "5. **End-to-End Learning**  \n",
        "   They enable end-to-end training, mapping raw inputs directly to outputs without requiring intermediate handcrafted processing steps.\n",
        "\n",
        "---\n",
        "\n",
        "## Difficulties in Designing Neural Networks\n",
        "\n",
        "1. **Architecture Design**  \n",
        "   Choosing the right model architecture—number of layers, types of layers (convolutional, recurrent, attention), and connections—is a complex and often domain-specific task.\n",
        "\n",
        "2. **Training Stability and Optimization**  \n",
        "   Neural networks involve non-convex optimization, which can be challenging due to issues like vanishing/exploding gradients, saddle points, and poor local minima.\n",
        "\n",
        "3. **Hyperparameter Tuning**  \n",
        "   Parameters such as learning rate, batch size, dropout rate, and weight decay have a significant impact on performance and require careful tuning, often through trial and error or automated search.\n",
        "\n",
        "4. **Overfitting and Generalization**  \n",
        "   Neural networks can easily overfit the training data due to their high capacity. Regularization techniques like dropout, weight decay, and data augmentation are necessary to improve generalization.\n",
        "\n",
        "5. **Data Requirements**  \n",
        "   Effective training usually requires large amounts of labeled data. In domains with limited labeled data, performance may suffer unless techniques like transfer learning or data augmentation are used.\n",
        "\n",
        "6. **Interpretability**  \n",
        "   Neural networks are often considered \"black boxes\" because understanding how and why they make certain decisions is difficult, which poses challenges in critical applications like healthcare, law, and finance.\n"
      ],
      "metadata": {
        "id": "N3ngWncHrc2z"
      }
    }
  ]
}